{"cells":[{"metadata":{"id":"wMDVc2WGzA82","trusted":true},"cell_type":"code","source":"import torch\n\nimport random\nimport numpy as np\nfrom transformers import BertTokenizer\n\n\nSEED = 321\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","execution_count":1,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":12573,"status":"ok","timestamp":1575773412340,"user":{"displayName":"Danny Toeun Kim","photoUrl":"https://lh4.googleusercontent.com/--snSJRfjKUA/AAAAAAAAAAI/AAAAAAAABA8/JS6ff2ex4nM/s64/photo.jpg","userId":"18139933724497343240"},"user_tz":0},"id":"E_g4GwtuzA87","outputId":"dee0282d-c66b-42fe-ef96-d08a99870bb7","trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\n\nprint('Loading Tokenizer...')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","execution_count":2,"outputs":[{"output_type":"stream","text":"Loading Tokenizer...\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f24fb67d545344c885813d0d2902ce5b"}},"metadata":{}}]},{"metadata":{"executionInfo":{"elapsed":633,"status":"ok","timestamp":1575763132991,"user":{"displayName":"Danny Toeun Kim","photoUrl":"https://lh4.googleusercontent.com/--snSJRfjKUA/AAAAAAAAAAI/AAAAAAAABA8/JS6ff2ex4nM/s64/photo.jpg","userId":"18139933724497343240"},"user_tz":0},"id":"DsN4ctVtzA8-","outputId":"a48e15a0-c33e-49b2-c6db-fc5dbc250acb","trusted":true},"cell_type":"code","source":"print('Vocab Size:',len(tokenizer.vocab))\n\ntokens = tokenizer.tokenize('This is a string to tokenize')\n\nprint('Tokenized list:', tokens) # Split sentence into tokens\n\nindexes = tokenizer.convert_tokens_to_ids(tokens) \n\nprint('Token IDS:', indexes) # print sentence mapped to tokens ids","execution_count":3,"outputs":[{"output_type":"stream","text":"Vocab Size: 30522\nTokenized list: ['this', 'is', 'a', 'string', 'to', 'token', '##ize']\nToken IDS: [2023, 2003, 1037, 5164, 2000, 19204, 4697]\n","name":"stdout"}]},{"metadata":{"executionInfo":{"elapsed":659,"status":"ok","timestamp":1575763139544,"user":{"displayName":"Danny Toeun Kim","photoUrl":"https://lh4.googleusercontent.com/--snSJRfjKUA/AAAAAAAAAAI/AAAAAAAABA8/JS6ff2ex4nM/s64/photo.jpg","userId":"18139933724497343240"},"user_tz":0},"id":"dkn4421ZzA9H","outputId":"ce6d598a-c3cf-42ab-effe-7a9a63ba7737","trusted":true},"cell_type":"code","source":"# CLS, SEP, PAD and UNK token\ninit_token = tokenizer.cls_token\neos_token = tokenizer.sep_token\npad_token = tokenizer.pad_token\nunk_token = tokenizer.unk_token\n\nprint(init_token, eos_token, pad_token, unk_token)\n\n# INDEX\ninit_token_idx = tokenizer.convert_tokens_to_ids(init_token)\neos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\npad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\nunk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)","execution_count":4,"outputs":[{"output_type":"stream","text":"[CLS] [SEP] [PAD] [UNK]\n","name":"stdout"}]},{"metadata":{"executionInfo":{"elapsed":486,"status":"ok","timestamp":1575763144922,"user":{"displayName":"Danny Toeun Kim","photoUrl":"https://lh4.googleusercontent.com/--snSJRfjKUA/AAAAAAAAAAI/AAAAAAAABA8/JS6ff2ex4nM/s64/photo.jpg","userId":"18139933724497343240"},"user_tz":0},"id":"5kn7tNNzzA9Q","outputId":"aff08c85-a821-4ef9-8d19-149b22445927","trusted":true},"cell_type":"code","source":"max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n\nprint(max_input_length) # 512\n\ndef tokenize_and_cut(sentence):\n    tokens = tokenizer.tokenize(sentence) \n    tokens = tokens[:max_input_length-2]\n    return tokens","execution_count":5,"outputs":[{"output_type":"stream","text":"512\n","name":"stdout"}]},{"metadata":{"id":"7_-IHUE8zA9V","trusted":true},"cell_type":"code","source":"from torchtext import data\n\nTEXT = data.Field(batch_first = True,\n                  use_vocab = False,\n                  tokenize = tokenize_and_cut, # try to replace it with 'spacy'\n                  preprocessing = tokenizer.convert_tokens_to_ids,\n                  init_token = init_token_idx,\n                  eos_token = eos_token_idx,\n                  pad_token = pad_token_idx,\n                  unk_token = unk_token_idx)\n\nLABEL = data.LabelField(dtype = torch.float)","execution_count":6,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","name":"stderr"}]},{"metadata":{"executionInfo":{"elapsed":194386,"status":"ok","timestamp":1575763344868,"user":{"displayName":"Danny Toeun Kim","photoUrl":"https://lh4.googleusercontent.com/--snSJRfjKUA/AAAAAAAAAAI/AAAAAAAABA8/JS6ff2ex4nM/s64/photo.jpg","userId":"18139933724497343240"},"user_tz":0},"id":"3ltzZdjRzA9Y","outputId":"40ca892a-69a1-4117-8c8f-4fdc39f9e5e4","trusted":true},"cell_type":"code","source":"from torchtext import datasets\n\ntrain_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n\ntrain_data, valid_data = train_data.split(split_ratio=0.8 ,random_state = random.seed(SEED))\n\nprint(f\"Number of training examples: {len(train_data)}\") # 20,000 examples\nprint(f\"Number of validation examples: {len(valid_data)}\") # 5,000 examples\nprint(f\"Number of testing examples: {len(test_data)}\") # 25,000 examples","execution_count":7,"outputs":[{"output_type":"stream","text":"downloading aclImdb_v1.tar.gz\n","name":"stdout"},{"output_type":"stream","text":"aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:02<00:00, 36.8MB/s]\n/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n","name":"stderr"},{"output_type":"stream","text":"Number of training examples: 20000\nNumber of validation examples: 5000\nNumber of testing examples: 25000\n","name":"stdout"}]},{"metadata":{"executionInfo":{"elapsed":560,"status":"ok","timestamp":1575763408718,"user":{"displayName":"Danny Toeun Kim","photoUrl":"https://lh4.googleusercontent.com/--snSJRfjKUA/AAAAAAAAAAI/AAAAAAAABA8/JS6ff2ex4nM/s64/photo.jpg","userId":"18139933724497343240"},"user_tz":0},"id":"qrXBP84TzA9g","outputId":"96532fb8-1588-4664-cf92-a5c16b91a24c","trusted":true},"cell_type":"code","source":"tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[0])['text'])\n\nprint(tokens)","execution_count":8,"outputs":[{"output_type":"stream","text":"['one', 'of', 'the', 'most', 'un', '##her', '##ald', '##ed', 'great', 'works', 'of', 'animation', '.', 'though', 'it', 'makes', 'the', 'most', 'sophisticated', 'use', 'of', 'the', '\"', 'cut', '-', 'out', '\"', 'method', 'of', 'animation', '(', 'a', 'la', '\"', 'south', 'park', '\"', ')', ',', 'the', 'real', 'talent', 'behind', '\"', 'twice', 'upon', 'a', 'time', '\"', 'are', 'the', 'vocal', 'characterization', '##s', ',', 'with', 'lorenzo', 'music', \"'\", 's', '(', 'carlton', 'from', 'tv', \"'\", 's', '\"', 'r', '##ho', '##da', '\"', ')', 'woody', 'allen', '-', 'is', '##h', 'ralph', '-', 'the', '-', 'all', '-', 'purpose', '-', 'animal', 'being', 'the', 'center', '##piece', '.', 'the', '\"', 'accidental', 'nightmare', '\"', 'sequence', 'is', 'doubt', '##less', 'one', 'of', 'the', 'best', 'pieces', 'of', 'animation', 'ever', 'filmed', '.']\n","name":"stdout"}]},{"metadata":{"executionInfo":{"elapsed":562,"status":"ok","timestamp":1575763412012,"user":{"displayName":"Danny Toeun Kim","photoUrl":"https://lh4.googleusercontent.com/--snSJRfjKUA/AAAAAAAAAAI/AAAAAAAABA8/JS6ff2ex4nM/s64/photo.jpg","userId":"18139933724497343240"},"user_tz":0},"id":"He7eGkbnzA9o","outputId":"792e3fd2-7433-4a45-89cf-d8955480ef1f","trusted":true},"cell_type":"code","source":"LABEL.build_vocab(train_data)\nprint('Number of Positive and Positive Sentiments:', LABEL.vocab.freqs)\n","execution_count":9,"outputs":[{"output_type":"stream","text":"Number of Positive and Positive Sentiments: Counter({'pos': 10009, 'neg': 9991})\n","name":"stdout"}]},{"metadata":{"id":"XCAH8YT6zA9s","trusted":true},"cell_type":"code","source":"BATCH_SIZE = 128\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('The model is being trained on:', device)\n\ntrain_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n    (train_data, valid_data, test_data), \n    batch_size = BATCH_SIZE, \n    device = device)","execution_count":10,"outputs":[{"output_type":"stream","text":"The model is being trained on: cuda\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","name":"stderr"}]},{"metadata":{"id":"k0IL387YzA9w"},"cell_type":"markdown","source":"## Build the Model"},{"metadata":{"id":"9uw1neunzA9z","trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Define Bert model\nbert = BertModel.from_pretrained('bert-base-uncased')\n\n\nclass BERTSentimentAnalysis(nn.Module):\n    \n    def __init__(self, bert, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n        \n        super().__init__()\n        \n        self.bert = bert\n        \n        embedding_dim = bert.config.to_dict()['hidden_size']\n        \n        self.rnn = nn.GRU(embedding_dim,\n                          hidden_dim,\n                          num_layers = n_layers,\n                          bidirectional = bidirectional,\n                          batch_first = True,\n                          dropout = 0 if n_layers < 2 else dropout)        \n        \n        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        \n    def forward(self, text):\n                \n        with torch.no_grad():\n            embedded = bert(text)[0]\n        \n        _, hidden = self.rnn(embedded)\n        \n        if self.rnn.bidirectional:\n            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n        else:\n            hidden = self.dropout(hidden[-1,:,:])\n                    \n\n        output = self.out(hidden)\n        \n        return output","execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63558cf327414121958571a84089a69a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbea613ee7cd4453aa4a7c2a17a65405"}},"metadata":{}}]},{"metadata":{"id":"jh35OycMzA91","trusted":true},"cell_type":"code","source":"HIDDEN_DIM = 256\nOUTPUT_DIM = 1\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.2\n\nmodel = BERTSentimentAnalysis(bert,\n                         HIDDEN_DIM,\n                         OUTPUT_DIM,\n                         N_LAYERS,\n                         BIDIRECTIONAL,\n                         DROPOUT)","execution_count":12,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":475,"status":"ok","timestamp":1575763439836,"user":{"displayName":"Danny Toeun Kim","photoUrl":"https://lh4.googleusercontent.com/--snSJRfjKUA/AAAAAAAAAAI/AAAAAAAABA8/JS6ff2ex4nM/s64/photo.jpg","userId":"18139933724497343240"},"user_tz":0},"id":"zMnnAsgezA93","outputId":"da78b3d7-77f9-411b-c7a9-5520e9d1cabd","trusted":true},"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')\n\nfor name, param in model.named_parameters():                \n    if name.startswith('bert'):\n        param.requires_grad = False\n        \n        \nprint(f'The model now has {count_parameters(model):,} trainable parameters')","execution_count":13,"outputs":[{"output_type":"stream","text":"The model has 112,241,409 trainable parameters\nThe model has 2,759,169 trainable parameters\n","name":"stdout"}]},{"metadata":{"id":"iH6oWmVqzA-B"},"cell_type":"markdown","source":"## Model Training\n"},{"metadata":{"id":"WBVtko6AzA-C","trusted":true},"cell_type":"code","source":"import torch.optim as optim\n\noptimizer = optim.Adam(model.parameters())\n\ncriterion = nn.BCEWithLogitsLoss()\n\nmodel = model.to(device)\ncriterion = criterion.to(device)","execution_count":14,"outputs":[]},{"metadata":{"id":"nqAmcoBazA-N","trusted":true},"cell_type":"code","source":"# Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n\ndef accuracy(preds, y):\n    #round predictions to the closest integer\n    rounded_preds = torch.round(torch.sigmoid(preds))\n    correct = (rounded_preds == y).float() #convert into float \n    acc = correct.sum() / len(correct)\n    return acc\n\n\ndef train(model, iterator, optimizer, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for batch in iterator:\n        \n        optimizer.zero_grad()\n        \n        predictions = model(batch.text).squeeze(1)\n        \n        loss = criterion(predictions, batch.label)\n        \n        acc = accuracy(predictions, batch.label)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\n\ndef evaluate(model, iterator, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n\n            predictions = model(batch.text).squeeze(1)\n            \n            loss = criterion(predictions, batch.label)\n            \n            acc = accuracy(predictions, batch.label)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","execution_count":15,"outputs":[]},{"metadata":{"id":"I21mL2lhzA-Q","trusted":true},"cell_type":"code","source":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","execution_count":16,"outputs":[]},{"metadata":{"executionInfo":{"elapsed":2213545,"status":"ok","timestamp":1575766828430,"user":{"displayName":"Danny Toeun Kim","photoUrl":"https://lh4.googleusercontent.com/--snSJRfjKUA/AAAAAAAAAAI/AAAAAAAABA8/JS6ff2ex4nM/s64/photo.jpg","userId":"18139933724497343240"},"user_tz":0},"id":"mJdVHGFXzA-S","outputId":"19432eb9-7f15-4ffa-afa4-c2a6ba2f7a99","trusted":true},"cell_type":"code","source":"N_EPOCHS = 5\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    print(\"start time:\", start_time)\n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n    print(\"-\" * 30)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n    end_time = time.time()\n    print(\"end time:\" ,end_time)\n    end_time = time.time()\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    print(epoch_mins, epoch_secs)\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), './New-Bert-model.pt')\n    \n    #print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')","execution_count":17,"outputs":[{"output_type":"stream","text":"start time: 1617205796.9786918\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","name":"stderr"},{"output_type":"stream","text":"------------------------------\nend time: 1617206253.1845224\n7 36\n\tTrain Loss: 0.446 | Train Acc: 78.33%\n\t Val. Loss: 0.297 |  Val. Acc: 88.05%\nstart time: 1617206254.4091597\n------------------------------\nend time: 1617206710.540974\n7 36\n\tTrain Loss: 0.257 | Train Acc: 89.53%\n\t Val. Loss: 0.242 |  Val. Acc: 90.14%\nstart time: 1617206712.0723462\n------------------------------\nend time: 1617207168.330224\n7 36\n\tTrain Loss: 0.224 | Train Acc: 91.11%\n\t Val. Loss: 0.249 |  Val. Acc: 90.04%\nstart time: 1617207168.3314047\n------------------------------\nend time: 1617207624.6007917\n7 36\n\tTrain Loss: 0.199 | Train Acc: 92.12%\n\t Val. Loss: 0.217 |  Val. Acc: 91.56%\nstart time: 1617207626.12074\n------------------------------\nend time: 1617208082.4959943\n7 36\n\tTrain Loss: 0.173 | Train Acc: 93.38%\n\t Val. Loss: 0.235 |  Val. Acc: 90.23%\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We can see that Neural Network starts to overfit the data after the 5th EPOCH"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Uncomment this cell if you want to release memory of CUDA \n\n# !pip install GPUtil\n\n# import torch\n# from GPUtil import showUtilization as gpu_usage\n# from numba import cuda\n\n# def free_gpu_cache():\n#     print(\"Initial GPU Usage\")\n#     gpu_usage()                             \n\n#     torch.cuda.empty_cache()\n\n#     cuda.select_device(0)\n#     cuda.close()\n#     cuda.select_device(0)\n\n#     print(\"GPU Usage after emptying the cache\")\n#     gpu_usage()\n\n# free_gpu_cache()  ","execution_count":18,"outputs":[]},{"metadata":{"id":"vTC59Ed4zA-U"},"cell_type":"markdown","source":"We'll load up the parameters that gave us the best validation loss and try these on the test set - which gives us our best results so far!"},{"metadata":{"executionInfo":{"elapsed":1371,"status":"ok","timestamp":1575774238773,"user":{"displayName":"Danny Toeun Kim","photoUrl":"https://lh4.googleusercontent.com/--snSJRfjKUA/AAAAAAAAAAI/AAAAAAAABA8/JS6ff2ex4nM/s64/photo.jpg","userId":"18139933724497343240"},"user_tz":0},"id":"EiykE9iezA-V","outputId":"244a5166-81a5-4e45-910e-70624256154c","trusted":true},"cell_type":"code","source":"model.load_state_dict(torch.load('./New-Bert-model.pt'))\n\ntest_loss, test_acc = evaluate(model, test_iterator, criterion)\n\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')","execution_count":19,"outputs":[{"output_type":"stream","text":"Test Loss: 0.196 | Test Acc: 92.25%\n","name":"stdout"}]},{"metadata":{"id":"pP3xrlpCzA-Y"},"cell_type":"markdown","source":"## Prediction Testing"},{"metadata":{"id":"Cnj1_oEezA-Z","trusted":true},"cell_type":"code","source":"def predict_sentiment(model, tokenizer, sentence):\n    model.eval()\n    tokens = tokenizer.tokenize(sentence)\n    tokens = tokens[:max_input_length-2]\n    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n    tensor = torch.LongTensor(indexed).to(device)\n    tensor = tensor.unsqueeze(0)\n    prediction = torch.sigmoid(model(tensor))\n    if prediction.item() > 0.5:\n        print('Movie review has a negative sentiment')\n    else:\n        print('Movie review has a positive sentiment')\n    return prediction.item()","execution_count":36,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reviews from critics are collected from Zack Snyder's cut of Justice League. Source - Rotten Tomatoes: https://www.rottentomatoes.com/m/zack_snyders_justice_league"},{"metadata":{"id":"db4pj2TizA-c","outputId":"a36d234a-66a5-47e9-922d-98483b67bad6","trusted":true},"cell_type":"code","source":"predict_sentiment(model, tokenizer, \"The new cut of Justice League is better than the theatre release of 'Justice League'. Unfortunately, that doesn’t make it a good movie\")\n","execution_count":37,"outputs":[{"output_type":"stream","text":"Movie review has a negative sentiment\n","name":"stdout"},{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"0.5217639207839966"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_sentiment(model, tokenizer, '''Zack Snyder's Justice League shows Snyder's evolution from a director known for emphasizing style over substance to a man who understands why we love superhero myths, whether they live in the DC Universe or Marvel's.''')","execution_count":41,"outputs":[{"output_type":"stream","text":"Movie review has a positive sentiment\n","name":"stdout"},{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"0.02805434726178646"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_sentiment(model, tokenizer, '''Let's not forget that the Snyder Cut was in part willed into being by a toxic social media campaign, mob rule begetting a mystical monocultural object that is finally much bigger than its ostensible creator.''')\n","execution_count":48,"outputs":[{"output_type":"stream","text":"Movie review has a negative sentiment\n","name":"stdout"},{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"0.826120913028717"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_sentiment(model, tokenizer, '''It's a miracle this film was released 4 years later! ZS's Justice League will be remembered forever in cinema history - pleasing current DC fans and attracting new ones!''')\n","execution_count":54,"outputs":[{"output_type":"stream","text":"Movie review has a positive sentiment\n","name":"stdout"},{"output_type":"execute_result","execution_count":54,"data":{"text/plain":"0.047379642724990845"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"You are more than welcome to download trained BERT Model and test it yourself!"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}